{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7107f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56512682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08033f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from src.model.modeling_char_encoder import LitCharEncoder\n",
    "from src.data.dataset import get_train_dataloader, get_dev_dataloader, get_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe0341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "DATASET_NAME = 'jwengr/C-LLM'\n",
    "SPACE_TOKEN = '[SEP]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "MINI_BATCH_SIZE=32\n",
    "N_BATCH = 1\n",
    "BASE_MODEL_NAME='google-bert/bert-base-multilingual-cased'\n",
    "EPOCHS=10\n",
    "LEARNING_RATE = 5e-5\n",
    "TRAIN_MAX_LENGTH=128\n",
    "VALID_MAX_LENGTH=128\n",
    "INFERENCE_SENTENCE_MAX_LENGTH=64\n",
    "INFERENCE_SENTENCE_MIN_LENGTH=32\n",
    "INFERENCE_SENTENCE_N_OVERLAP=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf1f57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7d03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_train_dataloader(DATASET_NAME, batch_size=MINI_BATCH_SIZE, max_length=TRAIN_MAX_LENGTH)\n",
    "dev_dl = get_dev_dataloader(DATASET_NAME, batch_size=MINI_BATCH_SIZE, max_length=VALID_MAX_LENGTH)\n",
    "test_dl = get_test_dataloader(DATASET_NAME, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91caff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2178: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lit_char_encoder = LitCharEncoder(\n",
    "    base_model_name=BASE_MODEL_NAME,\n",
    "    space_token=SPACE_TOKEN,\n",
    "    unk_token=UNK_TOKEN,\n",
    "    pad_token=PAD_TOKEN,\n",
    "    lr=LEARNING_RATE,\n",
    "    epochs=EPOCHS,\n",
    "    inference_sentence_max_length=INFERENCE_SENTENCE_MAX_LENGTH,\n",
    "    inference_sentence_min_length=INFERENCE_SENTENCE_MIN_LENGTH,\n",
    "    inference_sentence_n_overlap=INFERENCE_SENTENCE_N_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f92f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/charencoder',\n",
    "    filename=f\"{DATASET_NAME.split('/')[1]}/{BASE_MODEL_NAME.split('/')[1]}\"+\"-{epoch:02d}-{valid_loss:.4f}\",\n",
    "    every_n_epochs=1,\n",
    "    save_top_k=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41df54b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/fabric/connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    precision='bf16',\n",
    "    max_epochs=EPOCHS,\n",
    "    enable_checkpointing=True,\n",
    "    accumulate_grad_batches=N_BATCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251e869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n",
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type        | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | encoder | CharEncoder | 177 M  | train\n",
      "------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.898   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "233       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  92%|█████████▏| 9483/10354 [18:36<01:42,  8.49it/s, v_num=53] "
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_char_encoder, train_dl, dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f604e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2178: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lit_char_encoder = LitCharEncoder.load_from_checkpoint(\n",
    "    'checkpoints/charencoder/C-LLM/bert-base-multilingual-cased-epoch=00-valid_loss=0.0197.ckpt',\n",
    "    base_model_name=BASE_MODEL_NAME,\n",
    "    space_token=SPACE_TOKEN,\n",
    "    unk_token=UNK_TOKEN,\n",
    "    pad_token=PAD_TOKEN,\n",
    "    lr=LEARNING_RATE,\n",
    "    epochs=EPOCHS,\n",
    "    inference_sentence_max_length=INFERENCE_SENTENCE_MAX_LENGTH,\n",
    "    inference_sentence_min_length=INFERENCE_SENTENCE_MIN_LENGTH,\n",
    "    inference_sentence_n_overlap=INFERENCE_SENTENCE_N_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c627967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd26aaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n",
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  10%|█         | 2785/26731 [00:22<03:10, 125.39it/s]​说到这,车德钢觉得有必要为大家进行一下简单的科普,越野车上的所谓“差速锁”对于越野性能究竟有着怎样的提升。\n",
      "说到这，车德钢觉得有必要为大家进行一下简单的科普,越野车上的所谓[差速锁[对于越野性能究竟有着怎样的提升，\n",
      "Predicting DataLoader 0:  11%|█         | 2807/26731 [00:22<03:10, 125.37it/s]操控器方面,方向盘比较轻,灵敏度也高,转向也非常的精准,天气冷的话还可以提前进行远程开启空调,车内的中控屏是方便,特别的智能化,车内的360全景影像画面清晰,开起来信心非常的强,上手也比较快。\n",
      "操控器方面，方向盘比较轻,灵敏度也高,转向也非常的精准，天气冷的话还可以提前进行远程开启空调,车内的中控屏是方便,特别的智能化,车内的360全景影像画面清晰,开起来信心非常的强,上手也比较快，\n",
      "Predicting DataLoader 0:  13%|█▎        | 3346/26731 [00:26<03:06, 125.17it/s]六、合作生效的标准乙方将此页面打萤签字、盖章后邮政寄给甲方。\n",
      "六、合作生效的标准乙方将此页面打黑签字、盖章后邮政寄给甲方，\n",
      "Predicting DataLoader 0:  13%|█▎        | 3401/26731 [00:27<03:06, 125.29it/s]2)乙方有权获得甲方市畅宣传广告上的支持。\n",
      "2)乙方有权获得甲方市场宣传广告上的支持，\n",
      "Predicting DataLoader 0:  14%|█▍        | 3717/26731 [00:29<03:02, 125.96it/s]⑤每月财务,由甲方保管,乙方监管,每月核算签字后,分红。\n",
      "⑤每月财务,由甲方保管,乙方监管,每月核算签字后,分红，\n",
      "Predicting DataLoader 0:  14%|█▍        | 3807/26731 [00:30<03:01, 126.07it/s]*本合同含产品报价清单一份,经双方签字盖章确认,与合同正文具有相同的法律效力;\n",
      "*本合同含产品报价清单一份,经双方签字盖章确认,与合同正文具有相同的法律效力;\n",
      "Predicting DataLoader 0:  14%|█▍        | 3837/26731 [00:30<03:01, 126.11it/s]2.甲方希望审核时间为年月日,环境/职业健康安全管理体系的第一阶段审核时间为年月日。\n",
      "2.甲方希望审核时间为年月日，环境/职业健康安全管理体系的第一阶段审核时间为年月日，\n",
      "Predicting DataLoader 0:  15%|█▍        | 3983/26731 [00:31<03:00, 126.24it/s]二、管护要求及达标标准1、乙方应尽职尽责履行《用材林管护合同》所规定的管护义务。\n",
      "二、管护要求及达标标准1、乙方应尽职尽责履行《用材林管护合同》所规定的管护义务，\n",
      "Predicting DataLoader 0:  49%|████▉     | 13147/26731 [01:38<01:41, 133.31it/s]策略一：想要保住本钱必先舍得本钱；策略二：细分风险政策激活韩信这颗棋，细分风险政策也就是分化风险；策略三：单枪匹马夺回新收益；　\n",
      "策略一：想要保住本钱必先舍得本钱；策略二：细分风险政策激活韩信这颗棋，细分风险政策也就是分化风险；策略三：单枪匹马夺回新收益；\n",
      "Predicting DataLoader 0:  75%|███████▍  | 19931/26731 [02:37<00:53, 126.50it/s]只是一想到那些印记,做父母的变心如刀绞,陆妈妈没好气的道:“我不管你是谁,也不管你们在一起多⻓时间,总之你没有经过我们父母的允许就对我女儿做出这种事情,简直就是丧心 \n",
      "只是一想到那些印记,做父母的变心如刀绞，陆妈妈没好气的道:[我不管你是谁,也不管你们在一起多[时间,总之你没有经过我们父母的允许就对我女儿做出这种事情,简直就是丧心\n",
      "Predicting DataLoader 0:  86%|████████▌ | 22860/26731 [03:00<00:30, 126.95it/s]萧灵芸说着,对白狐兽道: “去吧,你的主人有没有能力要回你,就要看她的本事了。”\n",
      "萧灵芸说着，对白狐兽道:[去吧，你的主人有没有能力要回你，就要看她的本事了，[\n",
      "Predicting DataLoader 0:  89%|████████▉ | 23851/26731 [03:07<00:22, 127.28it/s]﻿该村截止2006年底,已实现通]水、电、路、电视、电话五通,无路灯。\n",
      "该村截至2006年底，已实现通牒水、电、路、电视、电话五通,无路灯，\n",
      "Predicting DataLoader 0:  97%|█████████▋| 25864/26731 [03:22<00:06, 127.92it/s]183351建筑物按照燃烧性能可分为哪几种?\n",
      "183351建筑物按照燃烧性能可分为哪几种?\n",
      "Predicting DataLoader 0:  99%|█████████▉| 26468/26731 [03:26<00:02, 128.04it/s]踥拼音:qiè  \n",
      "[拼音:qiè\n",
      "注音:ㄑㄧㄝˋ部首:足,部外笔画:8,总笔画:15五笔86&98:KHUV仓颉:RMYTV郑码:JISZ笔顺编号:251212141431531四角号码:60144UniCode:CJK \n",
      "注音:[[[[部首:足,部外笔画:8,总笔画:15五笔86&98:KHUV仓颉:RMYTV郑码:JISZ笔顺编号:251212141431531四角号码:60144UniCode:CJK\n",
      "统一汉字  \n",
      "统一汉字\n",
      "U+8E25--------------------------------------------------------------------------------● \n",
      "-+8E25---------------------------------------------------------------------------------\n",
      "踥qiè  ㄑㄧㄝˋ◎ 〔~蹀(dié)〕小步行走的样子,如“众~~而日进兮,美超远而逾迈。”\n",
      "[qiè[[[[◎〔~[(dié)〕小步行走的样子,如[众~~而日进兮,美超远而逾迈，[\n",
      "Predicting DataLoader 0: 100%|██████████| 26731/26731 [03:28<00:00, 128.07it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(lit_char_encoder, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2118311",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b0bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['在最近的福特资本市场日投资者介绍会上,首席产品平台和运营官'],\n",
       " 'sentence_noisy': ['在最近的福特资本市场日投资者介绍会上,首席产品平台和运营干'],\n",
       " 'category': ['car']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e2398de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在最近的福特资本市场日投资者介绍会上，首席产品平台和运营干']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2467f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783e3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjw1214_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
