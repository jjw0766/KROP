{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7107f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56512682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08033f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from src.model.modeling_char_encoder import LitCharEncoder\n",
    "from src.data.dataset import get_train_dataloader, get_dev_dataloader, get_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe0341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "DATASET_NAME = 'jwengr/C-LLM'\n",
    "SPACE_TOKEN = '[SEP]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "MINI_BATCH_SIZE=32\n",
    "N_BATCH = 1\n",
    "BASE_MODEL_NAME='google-bert/bert-base-multilingual-cased'\n",
    "EPOCHS=10\n",
    "LEARNING_RATE = 5e-5\n",
    "TRAIN_MAX_LENGTH=128\n",
    "VALID_MAX_LENGTH=128\n",
    "INFERENCE_SENTENCE_MAX_LENGTH=64\n",
    "INFERENCE_SENTENCE_MIN_LENGTH=32\n",
    "INFERENCE_SENTENCE_N_OVERLAP=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf1f57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7d03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_train_dataloader(DATASET_NAME, batch_size=MINI_BATCH_SIZE, max_length=TRAIN_MAX_LENGTH)\n",
    "dev_dl = get_dev_dataloader(DATASET_NAME, batch_size=MINI_BATCH_SIZE, max_length=VALID_MAX_LENGTH)\n",
    "test_dl = get_test_dataloader(DATASET_NAME, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91caff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2178: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lit_char_encoder = LitCharEncoder(\n",
    "    base_model_name=BASE_MODEL_NAME,\n",
    "    space_token=SPACE_TOKEN,\n",
    "    unk_token=UNK_TOKEN,\n",
    "    pad_token=PAD_TOKEN,\n",
    "    lr=LEARNING_RATE,\n",
    "    epochs=EPOCHS,\n",
    "    inference_sentence_max_length=INFERENCE_SENTENCE_MAX_LENGTH,\n",
    "    inference_sentence_min_length=INFERENCE_SENTENCE_MIN_LENGTH,\n",
    "    inference_sentence_n_overlap=INFERENCE_SENTENCE_N_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f92f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/charencoder',\n",
    "    filename=f\"{DATASET_NAME.split('/')[1]}/{BASE_MODEL_NAME.split('/')[1]}\"+\"-{epoch:02d}-{valid_loss:.4f}\",\n",
    "    every_n_epochs=1,\n",
    "    save_top_k=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41df54b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/fabric/connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    precision='bf16',\n",
    "    max_epochs=EPOCHS,\n",
    "    enable_checkpointing=True,\n",
    "    accumulate_grad_batches=N_BATCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251e869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n",
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type        | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | encoder | CharEncoder | 177 M  | train\n",
      "------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.898   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "233       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9483/10354 [18:36<01:42,  8.49it/s, v_num=53] "
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_char_encoder, train_dl, dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f604e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2178: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lit_char_encoder = LitCharEncoder.load_from_checkpoint(\n",
    "    'checkpoints/charencoder/C-LLM/bert-base-multilingual-cased-epoch=00-valid_loss=0.0197.ckpt',\n",
    "    base_model_name=BASE_MODEL_NAME,\n",
    "    space_token=SPACE_TOKEN,\n",
    "    unk_token=UNK_TOKEN,\n",
    "    pad_token=PAD_TOKEN,\n",
    "    lr=LEARNING_RATE,\n",
    "    epochs=EPOCHS,\n",
    "    inference_sentence_max_length=INFERENCE_SENTENCE_MAX_LENGTH,\n",
    "    inference_sentence_min_length=INFERENCE_SENTENCE_MIN_LENGTH,\n",
    "    inference_sentence_n_overlap=INFERENCE_SENTENCE_N_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c627967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd26aaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n",
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  10%|â–ˆ         | 2785/26731 [00:22<03:10, 125.39it/s]â€‹è¯´åˆ°è¿™,è½¦å¾·é’¢è§‰å¾—æœ‰å¿…è¦ä¸ºå¤§å®¶è¿›è¡Œä¸€ä¸‹ç®€å•çš„ç§‘æ™®,è¶Šé‡Žè½¦ä¸Šçš„æ‰€è°“â€œå·®é€Ÿé”â€å¯¹äºŽè¶Šé‡Žæ€§èƒ½ç©¶ç«Ÿæœ‰ç€æ€Žæ ·çš„æå‡ã€‚\n",
      "è¯´åˆ°è¿™ï¼Œè½¦å¾·é’¢è§‰å¾—æœ‰å¿…è¦ä¸ºå¤§å®¶è¿›è¡Œä¸€ä¸‹ç®€å•çš„ç§‘æ™®,è¶Šé‡Žè½¦ä¸Šçš„æ‰€è°“[å·®é€Ÿé”[å¯¹äºŽè¶Šé‡Žæ€§èƒ½ç©¶ç«Ÿæœ‰ç€æ€Žæ ·çš„æå‡ï¼Œ\n",
      "Predicting DataLoader 0:  11%|â–ˆ         | 2807/26731 [00:22<03:10, 125.37it/s]æ“æŽ§å™¨æ–¹é¢,æ–¹å‘ç›˜æ¯”è¾ƒè½»,çµæ•åº¦ä¹Ÿî°¯é«˜,è½¬å‘ä¹Ÿéžå¸¸çš„ç²¾å‡†,å¤©æ°”å†·çš„è¯è¿˜å¯ä»¥æå‰è¿›è¡Œè¿œç¨‹å¼€å¯ç©ºè°ƒ,è½¦å†…çš„ä¸­æŽ§î³½å±î°¯æ˜¯æ–¹ä¾¿,ç‰¹åˆ«çš„æ™ºèƒ½åŒ–,è½¦å†…çš„360å…¨æ™¯å½±åƒç”»é¢î°¯æ¸…æ™°,å¼€èµ·æ¥ä¿¡å¿ƒéžå¸¸çš„å¼º,ä¸Šæ‰‹ä¹Ÿæ¯”è¾ƒå¿«ã€‚\n",
      "æ“æŽ§å™¨æ–¹é¢ï¼Œæ–¹å‘ç›˜æ¯”è¾ƒè½»,çµæ•åº¦ä¹Ÿé«˜,è½¬å‘ä¹Ÿéžå¸¸çš„ç²¾å‡†ï¼Œå¤©æ°”å†·çš„è¯è¿˜å¯ä»¥æå‰è¿›è¡Œè¿œç¨‹å¼€å¯ç©ºè°ƒ,è½¦å†…çš„ä¸­æŽ§å±æ˜¯æ–¹ä¾¿,ç‰¹åˆ«çš„æ™ºèƒ½åŒ–,è½¦å†…çš„360å…¨æ™¯å½±åƒç”»é¢æ¸…æ™°,å¼€èµ·æ¥ä¿¡å¿ƒéžå¸¸çš„å¼º,ä¸Šæ‰‹ä¹Ÿæ¯”è¾ƒå¿«ï¼Œ\n",
      "Predicting DataLoader 0:  13%|â–ˆâ–Ž        | 3346/26731 [00:26<03:06, 125.17it/s]å…­ã€åˆä½œç”Ÿæ•ˆçš„æ ‡å‡†ä¹™æ–¹å°†æ­¤é¡µé¢æ‰“è¤î‹±ç­¾å­—ã€ç›–ç« åŽé‚®æ”¿å¯„ç»™ç”²æ–¹ã€‚\n",
      "å…­ã€åˆä½œç”Ÿæ•ˆçš„æ ‡å‡†ä¹™æ–¹å°†æ­¤é¡µé¢æ‰“é»‘ç­¾å­—ã€ç›–ç« åŽé‚®æ”¿å¯„ç»™ç”²æ–¹ï¼Œ\n",
      "Predicting DataLoader 0:  13%|â–ˆâ–Ž        | 3401/26731 [00:27<03:06, 125.29it/s]2)ä¹™æ–¹æœ‰æƒèŽ·å¾—ç”²æ–¹å¸‚ç•…î‹±å®£ä¼ å¹¿å‘Šä¸Šçš„æ”¯æŒã€‚\n",
      "2)ä¹™æ–¹æœ‰æƒèŽ·å¾—ç”²æ–¹å¸‚åœºå®£ä¼ å¹¿å‘Šä¸Šçš„æ”¯æŒï¼Œ\n",
      "Predicting DataLoader 0:  14%|â–ˆâ–        | 3717/26731 [00:29<03:02, 125.96it/s]î ‹î ‹î ‹î ‹î ‹â‘¤æ¯æœˆè´¢åŠ¡,ç”±ç”²æ–¹ä¿ç®¡,ä¹™æ–¹ç›‘ç®¡,æ¯æœˆæ ¸ç®—ç­¾å­—åŽ,åˆ†çº¢ã€‚\n",
      "â‘¤æ¯æœˆè´¢åŠ¡,ç”±ç”²æ–¹ä¿ç®¡,ä¹™æ–¹ç›‘ç®¡,æ¯æœˆæ ¸ç®—ç­¾å­—åŽ,åˆ†çº¢ï¼Œ\n",
      "Predicting DataLoader 0:  14%|â–ˆâ–        | 3807/26731 [00:30<03:01, 126.07it/s]*î ‹î ‹î ‹æœ¬åˆåŒå«äº§å“æŠ¥ä»·æ¸…å•ä¸€ä»½,ç»åŒæ–¹ç­¾å­—ç›–ç« ç¡®è®¤,ä¸ŽåˆåŒæ­£æ–‡å…·æœ‰ç›¸åŒçš„æ³•å¾‹æ•ˆåŠ›;\n",
      "*æœ¬åˆåŒå«äº§å“æŠ¥ä»·æ¸…å•ä¸€ä»½,ç»åŒæ–¹ç­¾å­—ç›–ç« ç¡®è®¤,ä¸ŽåˆåŒæ­£æ–‡å…·æœ‰ç›¸åŒçš„æ³•å¾‹æ•ˆåŠ›;\n",
      "Predicting DataLoader 0:  14%|â–ˆâ–        | 3837/26731 [00:30<03:01, 126.11it/s]2.ç”²æ–¹å¸Œæœ›å®¡æ ¸æ—¶é—´ä¸ºî ‹î ‹å¹´î ‹î ‹æœˆî ‹î ‹æ—¥,çŽ¯å¢ƒ/èŒä¸šå¥åº·å®‰å…¨ç®¡ç†ä½“ç³»çš„ç¬¬ä¸€é˜¶æ®µå®¡æ ¸æ—¶é—´ä¸ºî ‹î ‹î ‹å¹´î ‹î ‹æœˆî ‹æ—¥ã€‚\n",
      "2.ç”²æ–¹å¸Œæœ›å®¡æ ¸æ—¶é—´ä¸ºå¹´æœˆæ—¥ï¼ŒçŽ¯å¢ƒ/èŒä¸šå¥åº·å®‰å…¨ç®¡ç†ä½“ç³»çš„ç¬¬ä¸€é˜¶æ®µå®¡æ ¸æ—¶é—´ä¸ºå¹´æœˆæ—¥ï¼Œ\n",
      "Predicting DataLoader 0:  15%|â–ˆâ–        | 3983/26731 [00:31<03:00, 126.24it/s]î ‹äºŒã€ç®¡æŠ¤è¦æ±‚åŠè¾¾æ ‡æ ‡å‡†1ã€î ‹ä¹™æ–¹åº”å°½èŒå°½è´£å±¥è¡Œã€Šç”¨ææž—ç®¡æŠ¤åˆåŒã€‹æ‰€è§„å®šçš„ç®¡æŠ¤ä¹‰åŠ¡ã€‚\n",
      "äºŒã€ç®¡æŠ¤è¦æ±‚åŠè¾¾æ ‡æ ‡å‡†1ã€ä¹™æ–¹åº”å°½èŒå°½è´£å±¥è¡Œã€Šç”¨ææž—ç®¡æŠ¤åˆåŒã€‹æ‰€è§„å®šçš„ç®¡æŠ¤ä¹‰åŠ¡ï¼Œ\n",
      "Predicting DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 13147/26731 [01:38<01:41, 133.31it/s]ç­–ç•¥ä¸€ï¼šæƒ³è¦ä¿ä½æœ¬é’±å¿…å…ˆèˆå¾—æœ¬é’±ï¼›ç­–ç•¥äºŒï¼šç»†åˆ†é£Žé™©æ”¿ç­–æ¿€æ´»éŸ©ä¿¡è¿™é¢—æ£‹ï¼Œç»†åˆ†é£Žé™©æ”¿ç­–ä¹Ÿå°±æ˜¯åˆ†åŒ–é£Žé™©ï¼›ç­–ç•¥ä¸‰ï¼šå•æžªåŒ¹é©¬å¤ºå›žæ–°æ”¶ç›Šï¼›ã€€\n",
      "ç­–ç•¥ä¸€ï¼šæƒ³è¦ä¿ä½æœ¬é’±å¿…å…ˆèˆå¾—æœ¬é’±ï¼›ç­–ç•¥äºŒï¼šç»†åˆ†é£Žé™©æ”¿ç­–æ¿€æ´»éŸ©ä¿¡è¿™é¢—æ£‹ï¼Œç»†åˆ†é£Žé™©æ”¿ç­–ä¹Ÿå°±æ˜¯åˆ†åŒ–é£Žé™©ï¼›ç­–ç•¥ä¸‰ï¼šå•æžªåŒ¹é©¬å¤ºå›žæ–°æ”¶ç›Šï¼›\n",
      "Predicting DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 19931/26731 [02:37<00:53, 126.50it/s]åªæ˜¯ä¸€æƒ³åˆ°é‚£äº›å°è®°,åšçˆ¶æ¯çš„å˜å¿ƒå¦‚åˆ€ç»ž,é™†å¦ˆå¦ˆæ²¡å¥½æ°”çš„é“:â€œæˆ‘ä¸ç®¡ä½ æ˜¯è°,ä¹Ÿä¸ç®¡ä½ ä»¬åœ¨ä¸€èµ·å¤šâ»“æ—¶é—´,æ€»ä¹‹ä½ æ²¡æœ‰ç»è¿‡æˆ‘ä»¬çˆ¶æ¯çš„å…è®¸å°±å¯¹æˆ‘å¥³å„¿åšå‡ºè¿™ç§äº‹æƒ…,ç®€ç›´å°±æ˜¯ä¸§å¿ƒÂ \n",
      "åªæ˜¯ä¸€æƒ³åˆ°é‚£äº›å°è®°,åšçˆ¶æ¯çš„å˜å¿ƒå¦‚åˆ€ç»žï¼Œé™†å¦ˆå¦ˆæ²¡å¥½æ°”çš„é“:[æˆ‘ä¸ç®¡ä½ æ˜¯è°,ä¹Ÿä¸ç®¡ä½ ä»¬åœ¨ä¸€èµ·å¤š[æ—¶é—´,æ€»ä¹‹ä½ æ²¡æœ‰ç»è¿‡æˆ‘ä»¬çˆ¶æ¯çš„å…è®¸å°±å¯¹æˆ‘å¥³å„¿åšå‡ºè¿™ç§äº‹æƒ…,ç®€ç›´å°±æ˜¯ä¸§å¿ƒ\n",
      "Predicting DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 22860/26731 [03:00<00:30, 126.95it/s]è§çµèŠ¸è¯´ç€,å¯¹ç™½ç‹å…½é“:Â â€œåŽ»å§,ä½ çš„ä¸»äººæœ‰æ²¡æœ‰èƒ½åŠ›è¦å›žä½ ,å°±è¦çœ‹å¥¹çš„æœ¬äº‹äº†ã€‚â€\n",
      "è§çµèŠ¸è¯´ç€ï¼Œå¯¹ç™½ç‹å…½é“:[åŽ»å§ï¼Œä½ çš„ä¸»äººæœ‰æ²¡æœ‰èƒ½åŠ›è¦å›žä½ ï¼Œå°±è¦çœ‹å¥¹çš„æœ¬äº‹äº†ï¼Œ[\n",
      "Predicting DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 23851/26731 [03:07<00:22, 127.28it/s]ï»¿è¯¥æ‘æˆªæ­¢2006å¹´åº•,å·²å®žçŽ°é€š]æ°´ã€ç”µã€è·¯ã€ç”µè§†ã€ç”µè¯äº”é€š,æ— è·¯ç¯ã€‚\n",
      "è¯¥æ‘æˆªè‡³2006å¹´åº•ï¼Œå·²å®žçŽ°é€šç‰’æ°´ã€ç”µã€è·¯ã€ç”µè§†ã€ç”µè¯äº”é€š,æ— è·¯ç¯ï¼Œ\n",
      "Predicting DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 25864/26731 [03:22<00:06, 127.92it/s]183351î€å»ºç­‘ç‰©æŒ‰ç…§ç‡ƒçƒ§æ€§èƒ½å¯åˆ†ä¸ºå“ªå‡ ç§?\n",
      "183351å»ºç­‘ç‰©æŒ‰ç…§ç‡ƒçƒ§æ€§èƒ½å¯åˆ†ä¸ºå“ªå‡ ç§?\n",
      "Predicting DataLoader 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 26468/26731 [03:26<00:02, 128.04it/s]è¸¥æ‹¼éŸ³:qiÃ¨Â Â \n",
      "[æ‹¼éŸ³:qiÃ¨\n",
      "æ³¨éŸ³:ã„‘ã„§ã„Ë‹éƒ¨é¦–:è¶³,éƒ¨å¤–ç¬”ç”»:8,æ€»ç¬”ç”»:15äº”ç¬”86&98:KHUVä»“é¢‰:RMYTVéƒ‘ç :JISZç¬”é¡ºç¼–å·:251212141431531å››è§’å·ç :60144UniCode:CJKÂ \n",
      "æ³¨éŸ³:[[[[éƒ¨é¦–:è¶³,éƒ¨å¤–ç¬”ç”»:8,æ€»ç¬”ç”»:15äº”ç¬”86&98:KHUVä»“é¢‰:RMYTVéƒ‘ç :JISZç¬”é¡ºç¼–å·:251212141431531å››è§’å·ç :60144UniCode:CJK\n",
      "ç»Ÿä¸€æ±‰å­—Â Â \n",
      "ç»Ÿä¸€æ±‰å­—\n",
      "U+8E25--------------------------------------------------------------------------------â—Â \n",
      "-+8E25---------------------------------------------------------------------------------\n",
      "è¸¥qiÃ¨Â Â ã„‘ã„§ã„Ë‹â—ŽÂ ã€”~è¹€(diÃ©)ã€•å°æ­¥è¡Œèµ°çš„æ ·å­,å¦‚â€œä¼—~~è€Œæ—¥è¿›å…®,ç¾Žè¶…è¿œè€Œé€¾è¿ˆã€‚â€\n",
      "[qiÃ¨[[[[â—Žã€”~[(diÃ©)ã€•å°æ­¥è¡Œèµ°çš„æ ·å­,å¦‚[ä¼—~~è€Œæ—¥è¿›å…®,ç¾Žè¶…è¿œè€Œé€¾è¿ˆï¼Œ[\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26731/26731 [03:28<00:00, 128.07it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(lit_char_encoder, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2118311",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b0bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['åœ¨æœ€è¿‘çš„ç¦ç‰¹èµ„æœ¬å¸‚åœºæ—¥æŠ•èµ„è€…ä»‹ç»ä¼šä¸Š,é¦–å¸­äº§å“å¹³å°å’Œè¿è¥å®˜'],\n",
       " 'sentence_noisy': ['åœ¨æœ€è¿‘çš„ç¦ç‰¹èµ„æœ¬å¸‚åœºæ—¥æŠ•èµ„è€…ä»‹ç»ä¼šä¸Š,é¦–å¸­äº§å“å¹³å°å’Œè¿è¥å¹²'],\n",
       " 'category': ['car']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e2398de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['åœ¨æœ€è¿‘çš„ç¦ç‰¹èµ„æœ¬å¸‚åœºæ—¥æŠ•èµ„è€…ä»‹ç»ä¼šä¸Šï¼Œé¦–å¸­äº§å“å¹³å°å’Œè¿è¥å¹²']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2467f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783e3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjw1214_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
