seed: 42
cuda_visible_devices: "2"

# Dataset & Model
dataset_name: jwengr/PeOcrSanskritPreproc
base_model_name: google-bert/bert-base-multilingual-cased
input_chars: ""
target_chars: ""

# Special Token
space_token: '[SEP]'
unk_token: '[UNK]'
pad_token: '[PAD]'

# Training Hyperparameters
mini_batch_size: 32
n_batch: 1
epochs: 10
learning_rate: 0.0001
use_bntd: true

# Sequence Lengths
train_max_length: 128
valid_max_length: 128
inference_sentence_max_length: 64
inference_sentence_min_length: 32
inference_sentence_n_overlap: 3