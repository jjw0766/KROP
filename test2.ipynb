{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2901c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use full attn qwen3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/fabric/connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:701: Checkpoint directory /home/jjw1214/KROP/checkpoints/bind exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | bind | BIND | 596 M  | train\n",
      "--------------------------------------\n",
      "596 M     Trainable params\n",
      "0         Non-trainable params\n",
      "596 M     Total params\n",
      "2,384.208 Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "427       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 3/16009 [00:00<56:07,  4.75it/s, v_num=50]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjw1214/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from src.model.modeling_bind import LitBIND\n",
    "from src.data.dataset import get_train_dataloader, get_dev_dataloader, get_test_dataloader\n",
    "\n",
    "SEED=42\n",
    "DATASET_NAME = 'jwengr/PeOcrSanskritPreproc'\n",
    "MINI_BATCH_SIZE = 4\n",
    "N_BATCH = 8\n",
    "BASE_MODEL_NAME='Qwen/Qwen3-0.6B-Base'\n",
    "EPOCHS=10\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_BNTD=True\n",
    "TRAIN_MAX_LENGTH=128\n",
    "VALID_MAX_LENGTH=128\n",
    "INFERENCE_SENTENCE_MAX_LENGTH=64\n",
    "INFERENCE_SENTENCE_MIN_LENGTH=32\n",
    "INFERENCE_SENTENCE_N_OVERLAP=3\n",
    "\n",
    "L.seed_everything(SEED)\n",
    "\n",
    "train_dl = get_train_dataloader(DATASET_NAME, batch_size=MINI_BATCH_SIZE, max_length=TRAIN_MAX_LENGTH)\n",
    "dev_dl = get_dev_dataloader(DATASET_NAME, batch_size=MINI_BATCH_SIZE, max_length=VALID_MAX_LENGTH)\n",
    "test_dl = get_test_dataloader(DATASET_NAME, batch_size=MINI_BATCH_SIZE)\n",
    "\n",
    "lit_bind = LitBIND(\n",
    "    base_model_name=BASE_MODEL_NAME,\n",
    "    lr=LEARNING_RATE,\n",
    "    epochs=EPOCHS,\n",
    "    use_bntd=USE_BNTD,\n",
    "    inference_sentence_max_length=INFERENCE_SENTENCE_MAX_LENGTH,\n",
    "    inference_sentence_min_length=INFERENCE_SENTENCE_MIN_LENGTH,\n",
    "    inference_sentence_n_overlap=INFERENCE_SENTENCE_N_OVERLAP,\n",
    "    n_tokens_per_char=12\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/bind',\n",
    "    filename=f\"{DATASET_NAME.split('/')[1]}-{BASE_MODEL_NAME.split('/')[1]}-addbce-focalloss\"+\"-{epoch:02d}-{valid_loss:.4f}\",\n",
    "    monitor='valid_loss',\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    "    save_top_k=3,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    precision='bf16',\n",
    "    max_epochs=EPOCHS,\n",
    "    accumulate_grad_batches=N_BATCH\n",
    ")\n",
    "\n",
    "trainer.fit(lit_bind, train_dl, dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e2871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_bind.bind.tokenizer.input_chars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500dc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_bind.bind.tokenizer.target_chars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, sentence_noisy = ['ते ल म्बनावनती कुतो हेतोः कुतः पृथिव्याः साधिते', ' घटानधिकरणमित्या दितत्पुरुषस्यापि स्वघटकनामद्वयोपस्थाप्यार्थद्वयावच्छिन्नस्वघटकनामान्तरार्थविषयता कबोधजनकतया नोक्तनियम इत्यत आह ', 'अन्यन्तयाह पौरेतरध्नमुल्कापसव्यकरणं दिवाकरहिमांश्वोः', 'स्वगुणागज्या व्यासस्तत्र भलिप्ताः स्फुटः परिधिः'], ['ते ल म्बनावनती कुतो हेतोः कुतः पृथिव्याः साधिते', ' घटानधिकरणमित्या दितत्पुलपस्यापि स्वचटकनामद्वयोपस्थाप्यार्थद्वयावच्छिन्नस्वघटकनामान्तरार्थविषयता कवोधजनकतया नोक्तनियम हत्यत आह घ', 'अन्यन्तयाह पौरेतरध्नमुल्कापसव्यकरणं दिवाकरहिमांश्बोः', 'स्वगुणागज्या व्यासस्तत्र भलिप्ताः स्फुटः परिधिः']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204063a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ते ल म्बनावनती कुतो हेतोः कुतः पृथिव्याः साधिते',\n",
       " ' घटानधिकरणमित्या दितत्पुरुषस्यापि स्वघटकनामद्वयोपस्थाप्यार्थद्वयावच्छिन्नस्वघटकनामान्तरार्थविषयता कबोधजनकतया नोक्तनियम इत्यत आह ',\n",
       " 'अन्यन्तयाह पौरेतरध्नमुल्कापसव्यकरणं दिवाकरहिमांश्वोः',\n",
       " 'स्वगुणागज्या व्यासस्तत्र भलिप्ताः स्फुटः परिधिः']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111bfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ते ल म्बनावनती कुतो हेतोः कुतः पृथिव्याः साधिते',\n",
       " ' घटानधिकरणमित्या दितत्पुलपस्यापि स्वचटकनामद्वयोपस्थाप्यार्थद्वयावच्छिन्नस्वघटकनामान्तरार्थविषयता कवोधजनकतया नोक्तनियम हत्यत आह घ',\n",
       " 'अन्यन्तयाह पौरेतरध्नमुल्कापसव्यकरणं दिवाकरहिमांश्बोः',\n",
       " 'स्वगुणागज्या व्यासस्तत्र भलिप्ताः स्फुटः परिधिः']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = lit_bind.bind.tokenizer.batch_encode_char(sentence, {})[0].shape[1]\n",
    "sentence_noisy_len = lit_bind.bind.tokenizer.batch_encode_char(sentence_noisy, {})[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d942d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 350)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_len, sentence_noisy_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674df08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 168/16009 [00:02<04:31, 58.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     sentence_noisy_len = \u001b[43mlit_bind\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentence_noisy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m      5\u001b[39m     sentence_len = lit_bind.bind.tokenizer.batch_encode_char(batch[\u001b[33m'\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m'\u001b[39m], {})[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sentence_noisy_len!=sentence_noisy_len:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KROP/src/tokenizer/modeling_tokenizer.py:549\u001b[39m, in \u001b[36mBINDTokenizer.batch_encode_char\u001b[39m\u001b[34m(self, sentences, chars_dict)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences)):\n\u001b[32m    548\u001b[39m     input_ids[i] = input_ids[i] + (max_length-\u001b[38;5;28mlen\u001b[39m(input_ids[i])) * [\u001b[38;5;28mself\u001b[39m.base_tokenizer.eos_token_id]\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     attention_mask.append([\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_id!=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_id \u001b[38;5;129;01min\u001b[39;00m input_ids[i]])\n\u001b[32m    550\u001b[39m     token_type_ids[i] = token_type_ids[i] + (max_length-\u001b[38;5;28mlen\u001b[39m(token_type_ids[i])) * [\u001b[32m0\u001b[39m]\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    552\u001b[39m     torch.LongTensor(input_ids),\n\u001b[32m    553\u001b[39m     torch.LongTensor(attention_mask),\n\u001b[32m    554\u001b[39m     torch.LongTensor(token_type_ids)\n\u001b[32m    555\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1095\u001b[39m, in \u001b[36mSpecialTokensMixin.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1093\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(value) \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m(tok) \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m value]\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m         attr_as_tokens = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_without_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1096\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(attr_as_tokens) \u001b[38;5;28;01mif\u001b[39;00m attr_as_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/jjw1214_py312/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1093\u001b[39m, in \u001b[36mSpecialTokensMixin.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1091\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1092\u001b[39m     value = _special_tokens_map[key]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m(tok) \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m value]\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     attr_as_tokens = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key_without_id)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sentence_len\n",
    "from tqdm.auto import tqdm\n",
    "for batch in tqdm(train_dl):\n",
    "    sentence_noisy_len = lit_bind.bind.tokenizer.batch_encode_char(batch['sentence_noisy'], {})[0].shape[1]\n",
    "    sentence_len = lit_bind.bind.tokenizer.batch_encode_char(batch['sentence'], {})[0].shape[1]\n",
    "    if sentence_noisy_len!=sentence_noisy_len:\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68cb5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 386])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_bind.bind.tokenizer.batch_encode_char(batch['sentence_noisy'])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2da126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_noisy': ['अत्र किं पूर्वं साधुरुत साधुषु चरतीति संदेहः',\n",
       "  'व्याख्यायते सतां तुष्टयै शब्दशक्तिप्रकाशिका',\n",
       "  'किं तदिं विषयलक्षणमित्यत्राह अवभासमानतैवेति',\n",
       "  'पुनरबजदनिष्पत्तितुल्या जदरेखासरेखानिष्पत्तिः कल्पिता'],\n",
       " 'sentence': ['अत्र किं पूर्वं साधुरुत साधुषु चरतीति संदेहः',\n",
       "  'व्याख्यायते सतां तुष्टयै शब्दशक्तिप्रकाशिका',\n",
       "  'किं तदिं विषयलक्षणमित्यत्राह अवभासमानतैवेति',\n",
       "  'पुनरबजदनिष्पत्तितुल्या जदरेखासरेखानिष्पत्तिः कल्पिता']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjw1214_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
